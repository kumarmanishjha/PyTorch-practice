{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import operator\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "unknown_token = 'UNK'\n",
    "sentence_start_token = 'SENTENCE_START'\n",
    "sentence_end_token = 'SENTENCE_END'\n",
    "corpora_dir = \"C:\\\\Users\\\\Manish\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\state_union\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data..\n"
     ]
    }
   ],
   "source": [
    "print('Reading data..')\n",
    "\n",
    "#Read all file path in corpora directory\n",
    "\n",
    "file_list = []\n",
    "for subdir, dirs, files in os.walk(corpora_dir):\n",
    "    for file in files:\n",
    "        file_list.append(os.path.join(subdir, file))\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for files in file_list:\n",
    "    with open(files, 'r') as f:\n",
    "        try:\n",
    "            string = f.read().replace('\\n', '')\n",
    "            sentences.extend(nltk.sent_tokenize(string))\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"PRESIDENT HARRY S. TRUMAN'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS April 16, 1945Mr.\",\n",
       " 'Speaker, Mr. President, Members of the Congress:It is with a heavy heart that I stand before you, my friends and colleagues, in the Congress of the United States.Only yesterday, we laid to rest the mortal remains of our beloved President, Franklin Delano Roosevelt.',\n",
       " 'At a time like this, words are inadequate.',\n",
       " 'The most eloquent tribute would be a reverent silence.Yet, in this decisive hour, when world events are moving so rapidly, our silence might be misunderstood and might give comfort to our enemies.In His infinite wisdom, Almighty God has seen fit to take from us a great man who loved, and was beloved by, all humanity.No man could possibly fill the tremendous void left by the passing of that noble soul.',\n",
       " 'No words can ease the aching hearts of untold millions of every race, creed and color.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add sentence delimiters\n",
    "sentences = [sentence_start_token + \" \"+ sentence + \" \"+ sentence_end_token for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SENTENCE_START PRESIDENT HARRY S. TRUMAN'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS April 16, 1945Mr. SENTENCE_END\",\n",
       " 'SENTENCE_START Speaker, Mr. President, Members of the Congress:It is with a heavy heart that I stand before you, my friends and colleagues, in the Congress of the United States.Only yesterday, we laid to rest the mortal remains of our beloved President, Franklin Delano Roosevelt. SENTENCE_END',\n",
       " 'SENTENCE_START At a time like this, words are inadequate. SENTENCE_END',\n",
       " 'SENTENCE_START The most eloquent tribute would be a reverent silence.Yet, in this decisive hour, when world events are moving so rapidly, our silence might be misunderstood and might give comfort to our enemies.In His infinite wisdom, Almighty God has seen fit to take from us a great man who loved, and was beloved by, all humanity.No man could possibly fill the tremendous void left by the passing of that noble soul. SENTENCE_END',\n",
       " 'SENTENCE_START No words can ease the aching hearts of untold millions of every race, creed and color. SENTENCE_END']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words is 19005\n"
     ]
    }
   ],
   "source": [
    "#Count word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "\n",
    "print('Number of unique words is ' +str(len(word_freq.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab_size = len(word_freq.items())\n",
    "vocab = word_freq.most_common(vocab_size)\n",
    "\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(word, index) for index, word in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current vocabulary size is 19005\n",
      "The least frequent word in voabulary is  fromthe and appeared  1  times.\n"
     ]
    }
   ],
   "source": [
    "print('Current vocabulary size is ' +str(vocab_size))\n",
    "print('The least frequent word in voabulary is ', vocab[-1][0],\n",
    "     'and appeared ', vocab[-1][1], ' times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fromthe', 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manish\\Miniconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/ np.sum(np.exp(x), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt.truncate\n",
    "        \n",
    "        #Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        \n",
    "    #Forward propogation\n",
    "    def forward_prop(self, x):\n",
    "        #Total number of time steps\n",
    "        T = len(x)\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros((T, self.word_dim))\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        \n",
    "        #For each time step..\n",
    "        \n",
    "        for t in np.arange(T):\n",
    "            #Indexing U by x[t] which is same as multippplying U with a one-hot vector\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        #Perform forward prop\n",
    "        o, s =self.forward_prop(x)\n",
    "        return np.argmax(o, axis = 1)\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x, y)/N\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            correct_word_predictions = 0[np.arange(len(y[i])), y[i]]\n",
    "            L += -1* np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
